{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13c5ff854ff08180",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## **HW3 Problem 2 (15 points): Artifical Neural Networks [TA: Sogol Mansouri]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58e7d9549befcb26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1) Neural Network Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5a9ea2f81e0f5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, go to Tensorflow's [Neural Network Playground](https://playground.tensorflow.org/). This website is an interactive and exploratory visualization of how the features, number of layers, training time, etc, influence the classification boundries of an ANN. Right now, we'll only worry ourselves with *classification* problems.\n",
    "\n",
    "Play with the visualization, and then answer the following questions below.\n",
    "\n",
    "#### Scenarios\n",
    "\n",
    "1. Using the default network topology, try training the network with the different activation functions (ReLU, Tanh, Sigmoid, Linear). What effect does the activation function have on the training time? What effect does the activation function have on the shape of the classification boundries?\n",
    "2. Take a look at [this setup](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2,2&seed=0.21855&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). Train until the classification boundry converges. This is one of the rare cases where the nodes in an ANN can be (semi) interpreted. What do the nodes in the first hidden layer represent? What about the second hidden layer? How do you think the ANN uses these learned \"features\" to make a decision?\n",
    "\n",
    "#### Exploration\n",
    "For each of the following questions:\n",
    "* Make a prediction before you begin exploring and testing.\n",
    "* Include a link to your scenario.\n",
    "* Explain why you think this scenario has this property.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "3. Find a scenario where a simple model (fewer neurons) outperforms a complex model. (In regards to overfitting)\n",
    "4. Find a scenario where no hidden layers perform well.\n",
    "5. Find a scenario where a model with no hidden layers performs poorly no matter the features.\n",
    "6. Find a scenario where it takes a lot of training time to get a correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ANN-explore",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. [Answer] \n",
    "\n",
    "By testing, the training time for each activate function are:\n",
    "Tanh: 35 epochs\n",
    "ReLU: 40 epochs\n",
    "Sigmoid: 500 epochs\n",
    "Linear: boundary can't coverage\n",
    "\n",
    "For Training time:\n",
    "ReLU: Usually converges faster due to its non-saturating nature and efficient gradient propagation.\n",
    "Tanh and Sigmoid: Tend to converge slower, especially with deep networks, due to gradient saturation (vanishing gradients).\n",
    "Linear: Struggles with classification tasks as it cannot model non-linear relationships, often resulting in poor performance and longer training times without meaningful convergence.\n",
    "\n",
    "For Boundary:\n",
    "ReLU: Produces sharp, piecewise linear boundaries.\n",
    "Tanh: Creates smoother, more curved boundaries due to its non-linear nature.\n",
    "Sigmoid: Similar to Tanh but often less effective due to weaker gradients.\n",
    "Linear: Fails to create non-linear boundaries, resulting in linear separations that are ineffective for complex datasets.\n",
    "\n",
    "2. [Answer]\n",
    "3. [Answer]\n",
    "4. [Answer]\n",
    "5. [Answer]\n",
    "6. [Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9233095f9bf0595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) Training and Testing a Neural Network (Group)\n",
    "\n",
    "For this problem, you'll be looking at a subset of the [UCI ML hand-written digits datasets](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits), which contains images of hand-written digits: 10 classes where each class refers to a digit.\n",
    "\n",
    "Each data entry is a input matrix of 8x8 where each element is an integer in the range 0..16. The matrix is flattened in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2add9a421651bb2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this question, **you have enough experience to do the entire model pipeline yourself**. That means *loading the data, creating splits, scaling the data, training and tuning the model, and evaluating the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72eba8ac84354b45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c9b6a78fc7c131d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1: Load the data. Use `np.unique()` to check the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "df = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71b61c2fc389a33f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cdddfe25259e2bea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get a distribution of the class label (target)\n",
    "np.unique(df.target, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the data into X (feautres) and Y (class)\n",
    "\n",
    "Assign the variables below to split the dataset in to X (features) and Y (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "slip-feature-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = Y = None\n",
    "# Y should be target and X should be features\n",
    "#TODO: add your code here\n",
    "X = X[:200]\n",
    "Y = Y[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-057d09a1cf1ca024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3: Create your train/test split. Use the provided random_state.\n",
    "\n",
    "**Note**: You should use a `train_size` of 0.8, or 80%, and make sure to use the `random_state` to ensure test cases work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "slit-train-test",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = X_test = y_train = y_test = None\n",
    "#use a `train_size` of 0.8, or 80%, and make sure to use the `random_state` to ensure test cases work\n",
    "#TODO: add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "slit-train-test_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_train.shape == (160, 64)\n",
    "assert y_train.shape == (160,)\n",
    "assert X_test.shape == (40, 64)\n",
    "assert y_test.shape == (40,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed20d5db7f3ddc72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4: Use a [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to normalize the image data. \n",
    "\n",
    "Pixel data, like other data we've encountered, should often be scaled before classification. While in practice scaling image data can be more complex, in this exercise we'll continue to use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "Fit the scaler only the the training X features, and then apply it to both training and test X features. We do this because in practice, we wouldn't be able to see data in the test X, so it shouldn't affect feature transformation. We therefore only use X_train for feature transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "scale-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assign these variables the standardized training and test datasets\n",
    "X_stand_train = X_stand_test = None\n",
    "#TODO: add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "scale-data_test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Go through each attribute\n",
    "for i in range(X_stand_train.shape[1]):\n",
    "    # Calculate the mean of that attribute: it should be 0\n",
    "    np.testing.assert_almost_equal(np.mean(X_stand_train[:, i]), 0)\n",
    "    # Calculate the standard deviation: it should be 1\n",
    "    std = np.std(X_stand_train[:, i])\n",
    "    # However, if the std was already 0, standardization won't change that,\n",
    "    # so skip this case\n",
    "    if abs(std) < 0.01:\n",
    "        continue\n",
    "    np.testing.assert_almost_equal(std, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6451bd16b481009b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5:  Train an MLP with default hyperparameters.\n",
    "\n",
    "For the following, you'll be using sklearn's built in Multi-layer Perceptron classifier [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "Use the default hyperparams aside from `max_iter`. `max_iter` is how many iterations of training the ANN goes though until it manually stops. The default `max_iter=200` is too long for our data currently. \n",
    "\n",
    "**Use random_state as the random_states and max_iter=20**. The detault parameters will use a single hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4ac9e87863d5d366",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "mlp-train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf=None\n",
    "# Tip: if you pass your MLP the parameter verbose=True, you can see each iteration of its backpropagation\n",
    "#TODO: add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2722a12a4e946191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 6:  Evaluate the model on the test dataset using a confusion matrix and a classification report\n",
    "\n",
    "Like all classifiers, the MLP has a [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.predict) function that is used to make predictions on trianing or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5292fd4b6c7cdc73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "mlp-confusion-matrix",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the classifier and assign mlp_cm to the confusion matrix of the evaluation\n",
    "mlp_cm = None\n",
    "#TODO: add your code here\n",
    "mlp_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "mlp-confusion-matrix_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(mlp_cm, [[4, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 4, 0, 0, 0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 2, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 3, 0, 0, 0, 0],\n",
    "       [2, 0, 2, 0, 0, 1, 1, 0, 0, 2],\n",
    "       [0, 0, 0, 1, 0, 0, 0, 2, 0, 0],\n",
    "       [0, 0, 0, 0, 1, 2, 0, 0, 0, 2],\n",
    "       [0, 0, 1, 0, 0, 1, 0, 0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "mlp-report",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Similarly generate a classification report for the test dataset\n",
    "mlp_clf_report = None\n",
    "#TODO: add your code here\n",
    "print(mlp_clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "mlp-report_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert mlp_clf_report == classification_report(y_test, clf.predict(X_stand_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "mlp-report-train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For comparison, generate a classification report for the *training* dataset\n",
    "mlp_clf_report = None\n",
    "#TODO: add your code here\n",
    "print(mlp_clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "mlp-report-train-public",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert mlp_clf_report == '              precision    recall  f1-score   support\\n\\n           0       0.89      0.94      0.91        17\\n           1       0.93      0.87      0.90        15\\n           2       0.67      0.67      0.67        15\\n           3       1.00      0.95      0.97        19\\n           4       0.82      0.82      0.82        17\\n           5       0.89      0.94      0.91        17\\n           6       0.92      0.92      0.92        13\\n           7       0.94      1.00      0.97        17\\n           8       0.80      0.29      0.42        14\\n           9       0.67      1.00      0.80        16\\n\\n    accuracy                           0.85       160\\n   macro avg       0.85      0.84      0.83       160\\nweighted avg       0.86      0.85      0.84       160\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8915072f0111304b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How well did the classifier do? What digit did it do best on? Which digits did it confuse the most? Do you think the classifier is likely over-fitting, underfitting or neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d631135dc57ee3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2525c741f9cb9464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hyperparams**:\n",
    "\n",
    "ANNs have *a lot* of hyperparams. This can include simple things such as the number of layers and nodes, up to tuning the learning rate and the gradient descent algorithm used. \n",
    "\n",
    "This process can require a lot of experimentation and intution through experience, but it can be automated to some extent using hyperparameter tuning. When we have multiple hyperparameters, we use an approach called GridSearch, where we try all combinations of various hyperparameters to find the combination that works best.\n",
    "\n",
    "For the following, you will practice the hyperparamater tuning for the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) with sklearn's [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function, you should explore different combination of the following parameters:\n",
    "\n",
    "* `activation`: The activation function of the the ANN. Defaults to ReLU.\n",
    "* `max_iter`: The ANN will train iterations until either the loss stops improving by a specified threshold, or `max_iters` is reached. Warning: the more you increase this, the more the training time will take! Patience is a virtue.\n",
    "* `hidden_layer_sizes`: A tuple representing the structure of the hidden layers. For example, giving the tuple `(100,50)` means that there's two hidden layers: the first being of size 100, and the second being of size 50. The tuple (100,) would mean a single hidden layer of size 100.\n",
    "\n",
    "Normally we would try many more possible combinations (and larger networks), but we've kept the list short to reduce computation time.\n",
    "\n",
    "**Try different permutations of these hyperprams and see how it affects the classification scores of your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a25db6b11a11edba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import the library\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-783ed3693fc71b1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The parameter list you will explore\n",
    "parameters = {'activation':['logistic', 'relu'], 'max_iter':[5, 10], 'hidden_layer_sizes':[(50,),(20,)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9071631cfc9a1dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now it's your turn, first initialize an MLPClassifier, make sure to **use \"random_state\" as the random_states**, then feed the parameter list defined above as well as the training data (**use \"X_stand_train\"**) to GridSearchCV to create a classifier with the best combination of the parameters. To do so, it uses cross-validation within the training dataset, so you never have to peek at your test dataset. Then fit the final classifier to the whole standardized training dataset.\n",
    "\n",
    "**Note**: You should use cv=2 in your grid search, to reduce the number of folds tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "hyper_tuning_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign clf to the optimized (with grid search) MLP model\n",
    "# TIP: Again, if you want to track the trianing progress, try passing \"verbose = True\" to the MLP\n",
    "clf = None\n",
    "#TODO: add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see the parameters of the winning model of our grid search\n",
    "# This model is the one clf actually uses when you call clf.fit\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "hyper_tuning_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert list(clf.cv_results_['rank_test_score']) == [3, 2, 8, 6, 4, 1, 6, 5]\n",
    "np.testing.assert_almost_equal(round(clf.best_score_,4), 0.2312)\n",
    "assert clf.best_params_['hidden_layer_sizes'] == (50,)\n",
    "assert clf.best_index_ == 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b9e3ea0745f19e23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3154d0ff3ab7d37a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now you will use the estimator with the best found parameters to generate predictions (stored as \"y_pred\") on testing dataset, **remember to use \"X_stand_test\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "prediction_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = None\n",
    "#TODO: add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "prediction_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert list(confusion_matrix(y_test,y_pred)[0]) == [0, 0, 0, 0, 1, 0, 1, 0, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a38625450c81cccb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this toy example, we used a very limited set of hyperparmeters to reduce training time, and so our tuned model will actually do worse than our original. However, in practice, the tuned model will generally have better generalization to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
